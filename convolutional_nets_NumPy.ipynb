{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (coded in Python/NumPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| layer   | description    | depth      | dimensions                          | hyperparameters (other than length)  | \n",
    "|:-------:|:--------------:|:----------:|:-----------------------------------:|:------------------------------------:|\n",
    "| Layer 1 | Input          | 3          | 32 x 32                             |                                      |\n",
    "| Layer 2 | Convolution    | \\*10       | weights(3 x 5 x 5), output(32 x 32) | fieldsize=\\*5, stride=\\*1, pad=\\*2   |\n",
    "| Layer 3 | Pooling        | 10         | output(16 x 16)                     | fieldsize=\\*2, stride=\\*2, pad=\\*0   |\n",
    "| Layer 4 | Convolution    | \\*20       | weights(10 x 5 x 5), output(16 x 16)| (same as layer 2)                    |\n",
    "| Layer 5 | Pooling        | 20         | output(8 x 8)                       | (same as layer 3)                    |\n",
    "| Layer 6 | Fully-Connected|            | input (1280)  output (\\*100)        |                                      | \n",
    "| Layer 7 | Output         |            | input (100)  output(10)             |                                      |\n",
    "|         |                |\\* set by user                                    |                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CNN:\n",
    "    '''    \n",
    "    Parameters applicable to convolutional layers 2 & 4:\n",
    "       self.ksize         :  kernel size with respect to weight filters/receptive fields\n",
    "       self.stride        :  number of pixels moved by filter at one time\n",
    "       self.pad           :  amount of zero padding to surround border of input volume\n",
    "    \n",
    "    Weights filters (numbering based on whether L is a convolutional layer):\n",
    "       self.W2            :  layer 2 weights (depth(L), depth(L-1), ksize, ksize)\n",
    "       self.W4            :  layer 4 weights  \"\n",
    "       self.n_inpts       :  total input depth (images and neurons)\n",
    "       self.calfactor     :  factor to help calibrate variances between network layers\n",
    "    \n",
    "    Other variables accessible inside convolution functions:\n",
    "       self.Xcol2         :  'im2col' aligned data, associated with convolutional layer 2\n",
    "       self.Xcol4         :  'im2col' aligned data, associated with convolutional layer 4\n",
    "       self.images        :   cached input to convolutional layer 2\n",
    "       self.Z3            :   cached input to convolutional layer 4\n",
    "    '''   \n",
    "    def __init__(self):\n",
    "        self.ksize      = 5\n",
    "        self.stride     = 1\n",
    "        self.pad        = 2 \n",
    "        self.n_inpts    = 3+10+20+1280+100\n",
    "        self.calfactor  = np.sqrt(2.0/self.n_inpts) \n",
    "        self.W2         = np.random.randn(10, 3, self.ksize, self.ksize) * self.calfactor  \n",
    "        self.W4         = np.random.randn(20, 10, self.ksize, self.ksize) * self.calfactor\n",
    "        self.Xcol2      = []\n",
    "        self.Xcol4      = []\n",
    "        self.images     = []\n",
    "        self.Z3         = []\n",
    "        \n",
    "# instantiate\n",
    "m = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization of other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000                            # number of training examples in batch\n",
    "imap = list( range(N) )             # index of training examples\n",
    "imagesshape = (3,32,32)             # input shape (depth, height & width dims)\n",
    "Z2shape = (10,32,32)                # layer 2 output shape (depth, height & width dims)\n",
    "Z3shape = (10,16,16)                # layer 3 output shape \"\n",
    "Z4shape = (20,16,16)                # layer 4 output shape \"\n",
    "Z5shape = (20,8,8)                  # layer 5 output shape \"\n",
    "hiddenshape = (100)                 # layer 6 output shape (length of hidden layer)\n",
    "outputshape = (10)                  # layer 7 output shape (number of output classes) \n",
    "W6 = np.random.randn(1280, 100) * m.calfactor # layer 6 weights (input size, hidden size)\n",
    "W7 = np.random.randn(100, 10) * m.calfactor   # layer 7 weigths (hidden size, output size)\n",
    "b2 = np.zeros( (Z2shape[0], 1) )              # layer 2 bias parameters\n",
    "b4 = np.zeros( (Z4shape[0], 1) )              # layer 4 bias parameters\n",
    "b6 = np.zeros( (1, hiddenshape) )             # layer 6 bias parameters\n",
    "b7 = np.zeros( (1, outputshape) )             # layer 7 bias parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, fld_hgt, fld_wdth, padding, stride):\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - fld_hgt) % stride == 0\n",
    "    assert (W + 2 * padding - fld_hgt) % stride == 0\n",
    "    out_height = int((H + 2 * padding - fld_hgt) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - fld_wdth) / stride + 1)\n",
    "    i0 = np.repeat(np.arange(fld_hgt), fld_wdth)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(fld_wdth), fld_hgt * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(C), fld_hgt * fld_wdth).reshape(-1, 1)\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(x, fld_hgt, fld_wdth, padding, stride): \n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(x.shape, fld_hgt, fld_wdth, padding, stride)\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(fld_hgt * fld_wdth * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, fld_hgt, fld_wdth, padding, stride):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, fld_hgt, fld_wdth, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * fld_hgt * fld_wdth, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolve_forward(m, inpt, W, b, layer, padding=m.pad, stride=m.stride):\n",
    "    '''\n",
    "    Performs matrix multiplication of weights (flattened by depth col), with receptive fields \n",
    "    (flattened by depth cols & training examples).          (see comments, for example shapes)\n",
    "                                                             ---------------------------------'''\n",
    "    Wshape0, Wshape1, Wshape2, Wshape3 = W.shape                                # 10, 3, 5, 5\n",
    "    Xshape0, Xshape1, Xshape2, Xshape3 = inpt.shape                         # 1000, 3, 32, 32\n",
    "    Zshape2 = int( (Xshape2 - Wshape2 + 2 * padding) / stride + 1 )                      # 32\n",
    "    Zshape3 = int( (Xshape3 - Wshape3 + 2 * padding) / stride + 1 )                      # 32\n",
    "    Xcol = im2col_indices(inpt, Wshape2, Wshape3, padding, stride)        # 3*5*5, 32*32*1000\n",
    "    Wcol = W.reshape(Wshape0, -1)                                                 # 10, 3*5*5\n",
    "    Z = Wcol @ Xcol + b                                                      # 10, 32*32*1000 \n",
    "    Z = Z.reshape(Wshape0, Zshape2, Zshape3, Xshape0)                      # 10, 32, 32, 1000\n",
    "    if layer == 2: m.Xcol2 = Xcol\n",
    "    if layer == 4: m.Xcol4 = Xcol\n",
    "    return Z.transpose(3, 0, 1, 2)                                         # 1000, 10, 32, 32\n",
    "\n",
    "def conv_backward(m, inpt, layer, padding=m.pad, stride=m.stride):\n",
    "    '''  Input:         inpt - backprop_error(L)\n",
    "       Returns:        dW(L) - backprop_error(L) * X(L).T   \n",
    "                       db(L) - sum(backprop_error(L))\n",
    "               outp_err(L-1) - W(L).T * backprop_error(L) '''\n",
    "    if layer == 2: Xcol, W, cached_inpt = m.Xcol2, m.W2, m.images\n",
    "    if layer == 4: Xcol, W, cached_inpt = m.Xcol4, m.W4, m.Z3       \n",
    "    Wshape0, Wshape1, Wshape2, Wshape3 = W.shape                                # 10, 3, 5, 5\n",
    "    in_reshaped = inpt.transpose(1, 2, 3, 0).reshape(Wshape0, -1)               # 10, 1024000\n",
    "    dW = in_reshaped @ Xcol.T   \n",
    "    dW = dW.reshape(W.shape)\n",
    "    db = np.sum(inpt, axis=(0, 2, 3))\n",
    "    db = db.reshape(Wshape0, -1)\n",
    "    W_reshape = W.reshape(Wshape0, -1)                                               # 10, 75\n",
    "    outcol = W_reshape.T @ in_reshaped                                          # 75, 1024000\n",
    "    outp = col2im_indices(outcol, cached_inpt.shape, Wshape2, Wshape3,  \n",
    "                          padding=padding, stride=stride)                   # 1000, 3, 32, 32\n",
    "    return dW, db, outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### pooling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_forward(inpt, size=2, stride=2):\n",
    "    '''Input:    inpt      -  2D output from convolution layer, per neuron, per example\n",
    "       Returns:  outp      -  output to pooling layer (spatial dimensions cut in half)\n",
    "                 argmaxes  -  max indices with respect to 4x4 input fields'''\n",
    "    inshape0, inshape1, inshape2, inshape3 = inpt.shape            #example: 1000,10,32,32\n",
    "    outshape2 = int( (inshape2 - size) / stride + 1 )                                  #16\n",
    "    outshape3 = int( (inshape3 - size) / stride + 1 )                                  #16\n",
    "    in_reshaped = inpt.reshape(inshape0*inshape1, 1, inshape2, inshape3)    #10000,1,32,32\n",
    "    Xcol = im2col_indices(in_reshaped, size, size, padding=0, stride=stride)    #4,2560000\n",
    "    argmaxes = np.argmax(Xcol, axis=0)                                            #2560000\n",
    "    outp = Xcol[argmaxes, range(argmaxes.size)]                                   #2560000\n",
    "    argmaxes = argmaxes.reshape(outshape2, outshape3, inshape0, inshape1)   #16,16,1000,10    \n",
    "    outp = outp.reshape(outshape2, outshape3, inshape0, inshape1)           #16,16,1000,10\n",
    "    return outp.transpose(2, 3, 0, 1), argmaxes.transpose(2, 3, 0, 1)       #1000,10,16,16\n",
    "\n",
    "def maxpool_backward(argmaxes, inpt, outshape):\n",
    "    '''This function is used in place of 'im2col' related functions. \n",
    "        Input:   argmaxes  -  max indices with respect to each 2x2 output block\n",
    "                 inpt      -  output error backpropogated from network\n",
    "                 outshape  -  shape of output error to apply to preceeding layer\n",
    "        Each input value maps to a position in a 2x2 output block, per argmax.'''         \n",
    "    outp = np.zeros(outshape)\n",
    "    for i in range(inpt.shape[0]):                              \n",
    "        for j in range(inpt.shape[1]):                           \n",
    "            r, c = 0, 0                                         \n",
    "            for k in range(inpt.shape[2]):                      \n",
    "                for m in range(inpt.shape[3]):                  \n",
    "                    maxix = argmaxes[i][j][k][m]\n",
    "                    if maxix == 0:   outp[i][j][r,c]     = inpt[i][j][k][m]\n",
    "                    elif maxix == 1: outp[i][j][r,c+1]   = inpt[i][j][k][m]\n",
    "                    elif maxix == 2: outp[i][j][r+1,c]   = inpt[i][j][k][m]\n",
    "                    else:            outp[i][j][r+1,c+1] = inpt[i][j][k][m]\n",
    "                    c += 2\n",
    "                r, c = r+2, 0\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return z * (z > 0)\n",
    "    \n",
    "def Softmax(z):\n",
    "    ''' e^z1 / (e^z1 + e^z2 + e^z3)'''\n",
    "    z = z - np.max(z)     \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "def derivative_Softmax(z):\n",
    "    ''' ( e^z1 â‹… (e^z2 + e^z3) ) / (e^z1 + e^z2 + e^z3)^2 '''\n",
    "    z = z - np.max(z)\n",
    "    A = np.exp(z)\n",
    "    C = np.sum(np.exp(z))\n",
    "    B = C - A\n",
    "    return A * B / C**2\n",
    "\n",
    "def derivative_ReLU(z):\n",
    "    return 1. * (z > 0)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input data & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "def load(N):\n",
    "    '''original images shape: length:N, height:32, width:32, depth:3'''\n",
    "    import cifar_data as cf\n",
    "    d = cf.CifarDataManager()\n",
    "    return d.train.images[:N], d.train.labels[:N]\n",
    "imgs, labels = load(N)\n",
    "imgs = np.transpose( imgs, (0,3,1,2) )  # depth to precede height & width\n",
    "\n",
    "# preprocessing (mean subtraction)\n",
    "m.images = imgs - imgs.mean(axis=0)\n",
    "\n",
    "# produce numerical label values from binary vectors \n",
    "Y = []\n",
    "for i in range(N): \n",
    "    Y.append( np.argmax(labels[i]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 2.30170511133\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "numsteps = 1\n",
    "learnrate = 1\n",
    "penalty = 0.001  \n",
    "p = 0.5  # keep_probability, with respect to dropout\n",
    "\n",
    "# training loop\n",
    "for iteration in range(numsteps):\n",
    "\n",
    "    # forward pass layers 2 through 5\n",
    "    Z2 = ReLU( convolve_forward(m, m.images, m.W2, b2, layer=2) )\n",
    "    m.Z3, argmaxes3 = maxpool_forward(Z2)\n",
    "    Z4 = ReLU( convolve_forward(m, m.Z3, m.W4, b4, layer=4) )\n",
    "    Z5, argmaxes5 = maxpool_forward(Z4) \n",
    "\n",
    "    # forward pass fully-connected layer 6\n",
    "    mask6 = (np.random.rand(*W6.shape) < p) / p\n",
    "    Z5flat = np.zeros( (N, 20*8*8) )\n",
    "    for i in range(N): \n",
    "        Z5flat[i] = np.ndarray.flatten(Z5[i]) #flatten pooling output\n",
    "    Z6 = ReLU( np.dot( Z5flat, mask6*W6 ) + b6 )\n",
    "\n",
    "    # forward pass output layer 7\n",
    "    mask7 = (np.random.rand(*W7.shape) < p) / p   \n",
    "    Z7 = np.dot(Z6, mask7*W7) + b7 \n",
    "    Yhat = Softmax(Z7)\n",
    "    \n",
    "    # evaluate loss\n",
    "    logloss = -np.log( Yhat[imap, Y] ) #wrt yhats, wrt correct classes\n",
    "    loss = np.sum(logloss)/N\n",
    "    print ('Iteration', iteration, 'loss', loss)\n",
    "\n",
    "    # compute errors with respect to output probabilities\n",
    "    outerr7 = - ( labels - Yhat ) \n",
    "\n",
    "    # backward pass output layer 7\n",
    "    delta7 = np.zeros( (N, outputshape) )\n",
    "    for i in range(N):\n",
    "        delta7[i] = outerr7[i] * derivative_Softmax( Z7[i] ) \n",
    "    dW7 = np.dot( Z6.T, delta7 )\n",
    "    db7 = np.sum( delta7, axis=0, keepdims=True )\n",
    "    outerr6 = np.dot( delta7, (mask7*W7).T ) \n",
    "\n",
    "    # backward pass fully-connected layer 6\n",
    "    delta6 = outerr6 * derivative_ReLU(Z6)  \n",
    "    dW6 = np.dot( Z5flat.T, delta6 )\n",
    "    db6 = np.sum( delta6, axis=0, keepdims=True )\n",
    "    outerr5 = np.dot( delta6, (mask6*W6).T )\n",
    " \n",
    "    # backward pass pooling layer 5\n",
    "    outerr5 = outerr5.reshape( (N, *Z5shape) )  # unflatten\n",
    "    outerr4 = maxpool_backward(argmaxes5, inpt=outerr5, outshape=((N, *Z4shape)))\n",
    "\n",
    "    # backward pass convolutional layer 4\n",
    "    delta4 = outerr4 * derivative_ReLU(Z4)  \n",
    "    dW4, db4, outerr3 = conv_backward(m, inpt=delta4, layer=4)\n",
    "\n",
    "    # backward pass pooling layer 3\n",
    "    outerr2 = maxpool_backward(argmaxes3, inpt=outerr3, outshape=((N, *Z2shape)))\n",
    "\n",
    "    # backward pass convolutional layer 2\n",
    "    delta2 = outerr2 * derivative_ReLU(Z2)  \n",
    "    dW2, db2, outerr1 = conv_backward(m, inpt=delta2, layer=2)\n",
    "    \n",
    "    # regularization penalty\n",
    "    dW7 += penalty * W7\n",
    "    dW6 += penalty * W6\n",
    "    dW4 += penalty * m.W4\n",
    "    dW2 += penalty * m.W2    \n",
    "    \n",
    "    # parameter update\n",
    "    W7   += -learnrate * dW7\n",
    "    W6   += -learnrate * dW6\n",
    "    m.W4 += -learnrate * dW4\n",
    "    m.W2 += -learnrate * dW2\n",
    "    b7   += -learnrate * db7\n",
    "    b6   += -learnrate * db6\n",
    "    b4   += -learnrate * db4\n",
    "    b2   += -learnrate * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I would like to acknowledge Stanford CS231 http://cs231n.github.io/convolutional-networks/ for providing excellent materials on this topic, and to https://github.com/wiseodd/hipsternet for sharing python code. Also thanks to https://github.com/stephencwelch/Neural-Networks-Demystified, https://www.youtube.com/watch?v=bxe2T-V8XRs for an outstanding video series on introducing ordinary neural networks, forward/back propogation, and gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
