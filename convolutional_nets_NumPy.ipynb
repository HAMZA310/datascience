{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Nets in Python/NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| layer   | description    | length     | dimensions(shape)                   | hyperparameters                      | \n",
    "|:-------:|:--------------:|:----------:|:-----------------------------------:|:------------------------------------:|\n",
    "| Layer 1 | Input          | colorbar: 3| 3x32x32                             |                                      |\n",
    "| Layer 2 | Convolutional  | \\*32       | weights(32x3x5x5), output(32x32)    | fieldsize=\\*5, stride=\\*1, pad=2     |\n",
    "| Layer 3 | Pooling        | 32         | output(16x16)                       | fieldsize=\\*2, stride=\\*2, pad=0     |\n",
    "| Layer 4 | Convolutional  | \\*64       | weights(64x32x5x5), output(16x16)   | (same as layer 2)                    |\n",
    "| Layer 5 | Pooling        | 64         | output(8x8)                         | (same as layer 3)                    |\n",
    "| Layer 6 | Convolutional  | \\*128      | weights(128x64x5x5), output(8x8)    | (same as layer 2)                    |\n",
    "| Layer 7 | Pooling        | 128        | output(4x4)                         | (same as layer 3)                    |\n",
    "| Layer 8 | Fully-Connected|            | input (2048)  output (\\*256)        |                                      | \n",
    "| Layer 9 | Output         |            | input (256)  output(10)             |                                      |\n",
    "|         |                |            |                                     | \\* value set by user                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is coded in Python/NumPy without the use of Tensforflow or any other deep learning library (with the exception<br>of sklearn, for shuffling of 3D image data), with an intention for better understanding the details that pertain to convolutional<br>neural networks, including convolutional and pooling functions, activation functions (ReLU, Softmax and SVM), forward and <br>backward propogation with batch normalization and dropout, gradient check, mini-batch processing, and optimization<br>methods including SGD, SGD with momentum, RMS Prop, and Adam.<br><br>Contents (sections)\n",
    "<br>----------------------<br>\n",
    "model, batch and data classes<br>\n",
    "convolution and pooling functions, 'im2col'<br>\n",
    "activation and delta functions<br>\n",
    "batch normalization functions<br>\n",
    "forward and backward pass functions<br>\n",
    "gradient check<br>\n",
    "training (resumed)<br>\n",
    "optimization methods<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;notes on various methods<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;adam optimization<br>\n",
    "appendix<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;chart of forward/backward propogation<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;derivation of backpropogation formulas<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;notes pertaining to use of 'batchnorm' vs 'not using batchnorm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.utils import shuffle as skshuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self):       \n",
    "\n",
    "        # weights/filters - number assigned according to convolutional layer #\n",
    "        # dimensions (depthcolumn(L), depthcolumn(L-1), ksize, ksize) \n",
    "        self.ksize    = 5   # kernel size of weight filter / receptive field\n",
    "        np.random.seed(1)\n",
    "        self.W2       = 0.1 * np.random.randn( 32,  3, self.ksize, self.ksize) \n",
    "        self.W4       = 0.1 * np.random.randn( 64, 32, self.ksize, self.ksize) \n",
    "        self.W6       = 0.1 * np.random.randn(128, 64, self.ksize, self.ksize) \n",
    "        \n",
    "        # fully-connected weights (number assigned according to fully-connected layer #)\n",
    "        self.W8       = 0.1 * np.random.randn(2048, 256)  # input size,  hidden size\n",
    "        self.W9       = 0.1 * np.random.randn(256,  10)   # hidden size, output size\n",
    "        \n",
    "        # batchnorm parameters: 'gamma'(scale parameter), 'beta'(shift parameter) \n",
    "        self.gam2     = np.ones(  ( 32,32,32) )           # per spatial location \n",
    "        self.gam4     = np.ones(  ( 64,16,16) )           # \"    \n",
    "        self.gam6     = np.ones(  (128, 8, 8) )           # \"\n",
    "        self.gam8     = np.ones(  (1, self.W8.shape[1]) ) # per hidden neuron     \n",
    "        self.gam9     = np.ones(  (1, self.W9.shape[1]) ) # per output neuron      \n",
    "        self.beta2    = np.zeros( ( 32,32,32) )        \n",
    "        self.beta4    = np.zeros( ( 64,16,16) )      \n",
    "        self.beta6    = np.zeros( (128, 8, 8) )      \n",
    "        self.beta8    = np.zeros( (1, self.W8.shape[1]) )     \n",
    "        self.beta9    = np.zeros( (1, self.W9.shape[1]) )     \n",
    "        \n",
    "        #batchnorm variables: mean and variance moving averages\n",
    "        self.mu2      = np.zeros( ( 32,32,32) )            \n",
    "        self.mu4      = np.zeros( ( 64,16,16) )             \n",
    "        self.mu6      = np.zeros( (128, 8, 8) )            \n",
    "        self.mu8      = np.zeros( (1, self.W8.shape[1]) )            \n",
    "        self.mu9      = np.zeros( (1, self.W9.shape[1]) )             \n",
    "        self.var2     = np.zeros( ( 32,32,32) )                \n",
    "        self.var4     = np.zeros( ( 64,16,16) )             \n",
    "        self.var6     = np.zeros( (128, 8, 8) )            \n",
    "        self.var8     = np.zeros( (1, self.W8.shape[1]) )             \n",
    "        self.var9     = np.zeros( (1, self.W9.shape[1]) ) \n",
    "        \n",
    "        # misc\n",
    "        self.lmbda    = 1e-03     # L2 regularization penalty\n",
    "        self.numW     = 5         # number of layers with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch class \n",
    "class used for mini-batches, validation & testing, and gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch:  \n",
    "    def __init__(self, size=128):\n",
    "        self.images   = []                                                      # CIFAR data        \n",
    "        self.labels   = []                                # actual classes as binary vectors\n",
    "        self.Y        = []                                      # actual classes as integers\n",
    "        self.size     = size                   # size of mini-batch or validation/test batch\n",
    "        self.imap     = list( range(self.size) )                # index of training examples\n",
    "        \n",
    "        # variables stored during forward pass\n",
    "        self.Zmu2     = []   # dot product activity prior to normalization, minus batch mean\n",
    "        self.Zmu4     = []   # \"\n",
    "        self.Zmu6     = []   # \"\n",
    "        self.Zmu8     = []   # \"\n",
    "        self.Zmu9     = []   # \"\n",
    "        self.Zn2      = []                                #  dot product activity normalized\n",
    "        self.Zn4      = []                                #  \"\n",
    "        self.Zn6      = []                                #  \" \n",
    "        self.Zn8      = []                                #  \" \n",
    "        self.Zn9      = []                                #  \"  \n",
    "        self.sdi2     = []                                      # standard deviation inverse\n",
    "        self.sdi4     = []                                      # \"\n",
    "        self.sdi6     = []                                      # \"\n",
    "        self.sdi8     = []                                      # \"\n",
    "        self.sdi9     = []                                      # \"\n",
    "        self.imgshp   = (self.size,   3, 32, 32)                               # input shape   \n",
    "        self.A3shape  = (self.size,  32, 16, 16)             # shape of pooling layer output\n",
    "        self.A5shape  = (self.size,  64,  8,  8)             # \"                   \n",
    "        self.A7shape  = (self.size, 128,  4,  4)             # \"  \n",
    "        self.A7flat   = np.zeros( (self.size, 2048) )    # output flattened by train-example  \n",
    "        self.A8       = []                  # ReLU activated output to fully-connected layer\n",
    "        self.Xcol2    = []      # 'im2col' aligned data, associated with convolutional layer\n",
    "        self.Xcol4    = []      #  \"\n",
    "        self.Xcol6    = []      #  \"\n",
    "        self.Xcol3    = []            # 'im2col' aligned data, associated with pooling layer\n",
    "        self.Xcol5    = []            #  \"\n",
    "        self.Xcol7    = []            #  \"\n",
    "        self.U8       = []                # dropout mask applicable to fully-connected layer\n",
    "        self.U9       = []                # \"        \n",
    "        self.yHat     = []                          # Softmax activated output probabilities\n",
    "        self.SVMloss  = []                                   # loss matrix applicable to SVM\n",
    "        \n",
    "        # parameters applicable to convolutional layers 2, 4 and 6  \n",
    "        self.stride   =  1                    # number of pixels moved by filter at one time\n",
    "        self.pad      =  2       # amount of zero padding to surround border of input volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data class,  cifar classes\n",
    "data class is used to store train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images will be available for random batch training, except those\n",
      "allocated for testing and validation.\n",
      "X_train shape : (47952, 3, 32, 32)      y_train shape : (47952, 10)\n",
      "X_test shape  :  (1024, 3, 32, 32)      y_test shape  :  (1024, 10)\n",
      "X_val shape   :  (1024, 3, 32, 32)      y_val shape   :  (1024, 10)\n"
     ]
    }
   ],
   "source": [
    "class Data: \n",
    "    def __init__(self): \n",
    "        '''Training, Testing and Validation sets.'''\n",
    "        self.XTrn     = []\n",
    "        self.yTrn     = []\n",
    "        self.XTst     = []\n",
    "        self.yTst     = []\n",
    "        self.XVal     = []\n",
    "        self.yVal     = [] \n",
    "\n",
    "class CifarLoader(object):\n",
    "    def __init__(self, source_files):\n",
    "        self._source = source_files\n",
    "        self._i = 0\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def load(self):\n",
    "        data = [unpickle(f) for f in self._source]\n",
    "        images = np.vstack([d[\"data\"] for d in data])\n",
    "        n = len(images)\n",
    "        self.images = images.reshape(n, 3, 32, 32).astype(float) / 255\n",
    "        self.labels = one_hot(np.hstack([d[\"labels\"] for d in data]), 10)\n",
    "        return self\n",
    "\n",
    "class CifarDataManager(object):\n",
    "    def __init__(self):\n",
    "        self.train = CifarLoader([\"data_batch_{}\".format(i) for i in range(1, 6)]).load()\n",
    "        \n",
    "def one_hot(vec, vals=10):\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(os.path.join(DATA_PATH, file), 'rb') as fo:\n",
    "        u = pickle._Unpickler(fo)   \n",
    "        u.encoding = 'latin1'\n",
    "        dct = u.load()\n",
    "    return dct\n",
    "        \n",
    "def loadCifar(d):\n",
    "    images, labels = skshuffle(c.train.images, c.train.labels)\n",
    "    d.XTrn, d.yTrn = c.train.images[:47952],      c.train.labels[:47952]\n",
    "    d.XTst, d.yTst = c.train.images[47952:48976], c.train.labels[47952:48976]\n",
    "    d.XVal, d.yVal = c.train.images[48976:],      c.train.labels[48976:]\n",
    "\n",
    "DATA_PATH = \"CIFAR10_data/\"\n",
    "c = CifarDataManager()\n",
    "d = Data\n",
    "loadCifar(d)\n",
    "print ('All images will be available for random batch training, except those')\n",
    "print ('allocated for testing and validation.')\n",
    "print ('X_train shape :', d.XTrn.shape, '     y_train shape :', d.yTrn.shape)\n",
    "print ('X_test shape  : ', d.XTst.shape, '     y_test shape  : ', d.yTst.shape)\n",
    "print ('X_val shape   : ', d.XVal.shape, '     y_val shape   : ', d.yVal.shape)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution functions\n",
    "these functions do not include a bias term (refer to appendix section: 'batchnorm' vs 'not using batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolve_forward(b, X, W, L, test=False):\n",
    "    '''Forms receptive fields from input, and performs matrix multiplication of W (flattened \n",
    "       by depth), with receptive fields (flattened by depth & training examples). \n",
    "       Input:  X - input to convolutional layer (e.g., images, pooling layer output)\n",
    "               W - weights, L - network layer number    (see comments, for example shapes)'''\n",
    "    Xshape0, Xshape1, Xshape2, Xshape3 = X.shape  #(example batchsize: 1000)  1000, 3, 32, 32\n",
    "    Wshape0, Wshape1, Wshape2, Wshape3 = W.shape  #(example depthcolumn: 10)      10, 3, 5, 5\n",
    "    Zshape2 = int( (Xshape2 - Wshape2 + 2 * b.pad) / b.stride + 1 )                      # 32\n",
    "    Zshape3 = int( (Xshape3 - Wshape3 + 2 * b.pad) / b.stride + 1 )                      # 32\n",
    "    Xcol = im2col_indices(X, Wshape2, Wshape3, b.pad, b.stride)           # 3*5*5, 32*32*1000\n",
    "    Wcol = W.reshape(Wshape0, -1)                                                 # 10, 3*5*5\n",
    "    Z = Wcol @ Xcol                                                          # 10, 32*32*1000 \n",
    "    Z = Z.reshape(Wshape0, Zshape2, Zshape3, Xshape0)                      # 10, 32, 32, 1000\n",
    "    if not test:   \n",
    "        if L == 2: b.Xcol2 = Xcol                  \n",
    "        if L == 4: b.Xcol4 = Xcol\n",
    "        if L == 6: b.Xcol6 = Xcol\n",
    "    return Z.transpose(3,0,1,2)                                            # 1000, 10, 32, 32\n",
    "\n",
    "def convolve_backward(M, b, inpt, L):\n",
    "    '''  Input: backpropogating error applicable to convolutional layer (L)\n",
    "       Returns: gradient with respect to W, and output error(L-1)'''\n",
    "    if L == 2: Xcol, W, cached_inpt_shape = b.Xcol2, M.W2, b.imgshp\n",
    "    if L == 4: Xcol, W, cached_inpt_shape = b.Xcol4, M.W4, b.A3shape\n",
    "    if L == 6: Xcol, W, cached_inpt_shape = b.Xcol6, M.W6, b.A5shape    \n",
    "    Wshape0, Wshape1, Wshape2, Wshape3 = W.shape                                # 10, 3, 5, 5\n",
    "    inpt_reshaped = inpt.transpose(1, 2, 3, 0).reshape(Wshape0, -1)             # 10, 1024000\n",
    "    dW = inpt_reshaped @ Xcol.T   \n",
    "    dW = dW.reshape(W.shape)\n",
    "    W_reshape = W.reshape(Wshape0, -1)                                               # 10, 75\n",
    "    outcol = W_reshape.T @ inpt_reshaped                                        # 75, 1024000\n",
    "    outerr = col2im_indices(outcol, cached_inpt_shape, Wshape2, Wshape3,  \n",
    "                            padding=b.pad, stride=b.stride)                 # 1000, 3, 32, 32\n",
    "    return dW, outerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pooling functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_forward(b, inpt, L, test=False, size=2, stride=2, pad=0):\n",
    "    '''Input:    inpt              -  output (2D) from layer(L-1), per neuron, per example\n",
    "                 L                 -  network layer number (pooling layer)\n",
    "                 size, stride, pad -  preset parameters for 2x2 max pooling \n",
    "       Produces output: 1-to-1 neuron mapping from L-1 → spatial dims reduced by 50%'''\n",
    "    inshape0, inshape1, inshape2, inshape3 = inpt.shape               #example: 1000,10,32,32\n",
    "    outshape2 = int( (inshape2 - size) / stride + 1 )                                     #16\n",
    "    outshape3 = int( (inshape3 - size) / stride + 1 )                                     #16\n",
    "    in_reshaped = inpt.reshape(inshape0*inshape1, 1, inshape2, inshape3)       #10000,1,32,32\n",
    "    Xcol = im2col_indices(in_reshaped, size, size, pad, stride)                    #4,2560000\n",
    "    argmaxes = np.argmax(Xcol, axis=0)                                               #2560000\n",
    "    outp = Xcol[argmaxes, range(argmaxes.size)]                                      #2560000\n",
    "    outp = outp.reshape(outshape2, outshape3, inshape0, inshape1)              #16,16,1000,10 \n",
    "    if not test:\n",
    "        if L==3: b.Xcol3 = Xcol                      \n",
    "        if L==5: b.Xcol5 = Xcol \n",
    "        if L==7: b.Xcol7 = Xcol \n",
    "    return outp.transpose(2,3,0,1)                                             #1000,10,16,16\n",
    "               \n",
    "def maxpool_backward(b, inpt, L, size=2, stride=2, pad=0):\n",
    "    ''' Output_err(L) maps to Output_err(L-1).  Each value maps to position in 2x2 block.\n",
    "        A(L-1) from fwd-pass was not cached, however Zn(L-1) has an identical shape.'''\n",
    "    if L==7: dXcol, args, cach_in = np.zeros_like(b.Xcol7), np.argmax(b.Xcol7, axis=0), b.Zn6    \n",
    "    if L==5: dXcol, args, cach_in = np.zeros_like(b.Xcol5), np.argmax(b.Xcol5, axis=0), b.Zn4\n",
    "    if L==3: dXcol, args, cach_in = np.zeros_like(b.Xcol3), np.argmax(b.Xcol3, axis=0), b.Zn2\n",
    "    inpt_flat = inpt.transpose(2, 3, 0, 1).ravel()                   #16,16,1000,10 → 2560000\n",
    "    dXcol[args, range(args.size)] = inpt_flat                                      #4,2560000\n",
    "    n, d, w, h = cach_in.shape                                                 #1000,10,32,32\n",
    "    shape_ = n*d, 1, h, w                                                      #10000,1,32,32\n",
    "    outerr = col2im_indices(dXcol, shape_, size, size, pad, stride)            \n",
    "    if L == 7:  return outerr.reshape(cach_in.shape)\n",
    "    if L == 5:  return outerr.reshape(cach_in.shape)\n",
    "    if L == 3:  return outerr.reshape(cach_in.shape)                           #1000,10,32,32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col functions\n",
    "supports convolution and pooling functions utilizing fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im2col_indices(Xshape, field_height, field_width, padding, stride):\n",
    "    Xshape0, Xshape1, Xshape2, Xshape3 = Xshape\n",
    "    assert (Xshape2 + 2 * padding - field_height) % stride == 0\n",
    "    assert (Xshape3 + 2 * padding - field_height) % stride == 0\n",
    "    output_height = int((Xshape2 + 2 * padding - field_height) / stride + 1)\n",
    "    output_width = int((Xshape3 + 2 * padding - field_width) / stride + 1)\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, Xshape1)\n",
    "    i1 = stride * np.repeat(np.arange(output_height), output_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * Xshape1)\n",
    "    j1 = stride * np.tile(np.arange(output_width), output_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = np.repeat(np.arange(Xshape1), field_height * field_width).reshape(-1, 1)\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding, stride): \n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "    cols = X_padded[:, k, i, j]\n",
    "    Xshape1= X.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * Xshape1, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, Xshape, field_height, field_width, padding, stride):\n",
    "    Xshape0, Xshape1, Xshape2, Xshape3 = Xshape\n",
    "    Xshape2_pd, Xshape3_pd = Xshape2 + 2 * padding, Xshape3 + 2 * padding\n",
    "    X_padded = np.zeros((Xshape0, Xshape1, Xshape2_pd, Xshape3_pd), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(Xshape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(Xshape1 * field_height * field_width, -1, Xshape0)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return X_padded\n",
    "    return X_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation and delta functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def d_ReLU(outerr, z):\n",
    "    '''Input:   outerr - error with respect to activity z   \n",
    "                z      - activity cached from forward pass  \n",
    "       Returns backpropogating error δ.'''\n",
    "    backerr = outerr\n",
    "    backerr[z < 0] = 0\n",
    "    return backerr\n",
    "    \n",
    "def Leaky_ReLU(z, a=1e-3):\n",
    "    '''Same as ReLU, except a small negative is returned instead of 0.'''\n",
    "    return np.maximum(a*z, z) \n",
    "\n",
    "def d_Leaky_ReLU(outerr, z, a=1e-3):\n",
    "    ''' Returns backpropogating error δ, where σ is 'Leaky_ReLU' '''\n",
    "    backerr = outerr\n",
    "    backerr[z < 0] *= a    \n",
    "    return backerr\n",
    "        \n",
    "def Softmax(z):\n",
    "    ''' Returns output probilities adding up to 1.0 for each training example. '''\n",
    "    z = z - np.max(z)  \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "def d_Softmax(b):\n",
    "    ''' Returns output probabilities for each training example, where an adjustment is\n",
    "        made to the probabilities associated with correct class: p_k − 𝟙 where y_i = k. \n",
    "        Result may be used as backpropogating error, δ. '''\n",
    "    backerr = b.yHat\n",
    "    backerr[b.imap, b.Y] -= 1 \n",
    "    return backerr\n",
    "\n",
    "def SVM(b, s):\n",
    "    '''  Computes SVM loss matrix: \"SVM_loss_i = ∑ max(0,  s_j - s_yi + d)\"  \n",
    "          s_j: score associated with incorrect class;   i: training example \n",
    "         s_yi: score associated with correct class;     d: margin, e.g., 1  '''\n",
    "    loss_matrix = np.maximum(0,  s - s[b.imap, b.Y][:, np.newaxis] + 1) \n",
    "    loss_matrix[b.imap, b.Y] = 0 # replace correct score positions with 0\n",
    "    return loss_matrix\n",
    "    \n",
    "def d_SVM(b):\n",
    "    ''' Initialize error matrix from (cached) SVM loss. Binarize matrix by assigning\n",
    "        an indicator value of 1 to the positions that did not contribute to the desired \n",
    "        margin. Sum each training example, then subtract each sum from the position \n",
    "        associated with the correct class. Result may be used as backpropogating error, δ.'''\n",
    "    backerr = b.SVMloss.copy()\n",
    "    backerr[backerr > 0] = 1\n",
    "    backerr[b.imap, b.Y] = - np.sum(backerr, axis=1)\n",
    "    return backerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch normalization functions\n",
    "with chart of batchnorm forward and backward steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "             ⇓                -----------------                      \n",
    "      BATCHNORM FORWARD       |adjusted backerr|         δ2 = dZmuA + dZmuB + dZmuC         \n",
    "             ⇓                -----------------                     ↑ \n",
    "                              |   calculate    |        dZmuC = (d_muA + d_muB) / N\n",
    "      mu = 1/N * sum(Z)       |     mean,      |   <dmu is embedded in d_mu B&A backsteps>  \n",
    "        Zmu = Z - mu          |     & Zmu      |  <dZmu is embedded in dZmu B&A backsteps>       \n",
    "             ⇓                -----------------                     ↑ \n",
    "   var = 1/N * sum(Zmu^2)     |   calculate    |        dZmuB = 2 * Zmu * dvar / N\n",
    "   stdinv = 1 / sqrt(var)     |    variance    |       d_muB = mean(-2 * Zmu) * dvar \n",
    "                              |                |  dvar = sum(dZnorm*Zmu) * -.5 * stdinv^3     \n",
    "             ⇓                -----------------                     ↑  \n",
    "    Znorm = Zmu * stdinv      |   normalize    |          dZmuA = dZnorm * stdinv\n",
    "                              |                |     d_muA = sum(-1 * dZnorm * stdinv)\n",
    "             ⇓                -----------------                     ↑  \n",
    "                              |                |            dZnorm = δ1 * gamma\n",
    " out = gamma * Znorm + beta   |apply parameters|         dgamma = sum(δ1 * Znorm)           \n",
    "                              |                |             dbeta = sum(δ1)         \n",
    "             ⇓                -----------------                     ↑                   \n",
    " C * moving + (1 - C) * new   |  update moving |        n/a, no backstep applicable              \n",
    "                              | averages-mu,var|           \n",
    "             ⇓                -----------------                     ↑ \n",
    "                  ⥤                                         BATCHNORM BACKWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm_forward(M, b, Z, L, test=False, const=0.9, eps=1e-8):\n",
    "    ''' Batch normalization forward steps, as noted in chart.'''\n",
    "    if L==2: mu, var, gamma, beta = M.mu2, M.var2, M.gam2, M.beta2  \n",
    "    if L==4: mu, var, gamma, beta = M.mu4, M.var4, M.gam4, M.beta4  \n",
    "    if L==6: mu, var, gamma, beta = M.mu6, M.var6, M.gam6, M.beta6 \n",
    "    if L==8: mu, var, gamma, beta = M.mu8, M.var8, M.gam8, M.beta8  \n",
    "    if L==9: mu, var, gamma, beta = M.mu9, M.var9, M.gam9, M.beta9\n",
    "    if not test:\n",
    "        N = b.size\n",
    "        mu_new = 1. / N * np.sum(Z, axis=0) \n",
    "        Zmu = Z - mu_new                          \n",
    "        var_new = 1. / N * np.sum(Zmu**2, axis=0)                   \n",
    "        stdinv = 1. / np.sqrt(var_new + eps)                                     \n",
    "        Znorm = Zmu * stdinv\n",
    "        mov_mu  = const * mu  + (1. - const) * mu_new     \n",
    "        mov_var = const * var + (1. - const) * var_new    \n",
    "        if L==2: b.Zn2, b.Zmu2, b.sdi2, M.mu2, M.var2 = Znorm, Zmu, stdinv, mov_mu, mov_var\n",
    "        if L==4: b.Zn4, b.Zmu4, b.sdi4, M.mu4, M.var4 = Znorm, Zmu, stdinv, mov_mu, mov_var\n",
    "        if L==6: b.Zn6, b.Zmu6, b.sdi6, M.mu6, M.var6 = Znorm, Zmu, stdinv, mov_mu, mov_var\n",
    "        if L==8: b.Zn8, b.Zmu8, b.sdi8, M.mu8, M.var8 = Znorm, Zmu, stdinv, mov_mu, mov_var\n",
    "        if L==9: b.Zn9, b.Zmu9, b.sdi9, M.mu9, M.var9 = Znorm, Zmu, stdinv, mov_mu, mov_var\n",
    "        return gamma * Znorm + beta\n",
    "    else:\n",
    "        return gamma * (Z - mu) / np.sqrt(var + eps) + beta\n",
    "    \n",
    "def batchnorm_backward(M, b, backerr1, L):\n",
    "    '''Backward batch normalization steps, as noted in chart.''' \n",
    "    if L==9: Zn, Zmu, stdinv, gamma = b.Zn9, b.Zmu9, b.sdi9, M.gam9\n",
    "    if L==8: Zn, Zmu, stdinv, gamma = b.Zn8, b.Zmu8, b.sdi8, M.gam8\n",
    "    if L==6: Zn, Zmu, stdinv, gamma = b.Zn6, b.Zmu6, b.sdi6, M.gam6\n",
    "    if L==4: Zn, Zmu, stdinv, gamma = b.Zn4, b.Zmu4, b.sdi4, M.gam4\n",
    "    if L==2: Zn, Zmu, stdinv, gamma = b.Zn2, b.Zmu2, b.sdi2, M.gam2\n",
    "    N = b.size\n",
    "    dbeta = np.sum(backerr1, axis=0)                 \n",
    "    dgamma = np.sum(backerr1 * Zn, axis=0)              \n",
    "    dZnorm = backerr1 * gamma\n",
    "    dvar = np.sum(dZnorm * Zmu, axis=0) * -.5 * stdinv**3 \n",
    "    d_muA = np.sum(dZnorm * -1 * stdinv, axis=0)\n",
    "    d_muB = np.mean(-2 * Zmu, axis=0) * dvar\n",
    "    dZmuA = dZnorm * stdinv    \n",
    "    dZmuB = 2 * Zmu * dvar / N\n",
    "    dZmuC = (d_muA + d_muB) / N\n",
    "    backerr2 = dZmuA + dZmuB + dZmuC\n",
    "    return backerr2, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### primary forward and backward pass functions; dropout\n",
    "see appendix for chart of forward and backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_forward(b, Zn, L, p=0.5):\n",
    "    '''Applies dropout to Z-normalized by multiplying it with droput mask, U.'''\n",
    "    if L == 8:\n",
    "        b.U8 = ( np.random.rand(*Zn.shape) < p ) / p\n",
    "        return Zn * b.U8\n",
    "    if L == 9:\n",
    "        b.U9 = ( np.random.rand(*Zn.shape) < p ) / p\n",
    "        return Zn * b.U9 \n",
    "        \n",
    "def forward(M, b, f, drp=True, test=False):\n",
    "    ''' Forward steps as noted in chart (see appendix).  Input \"Softmax or \"SVM. '''\n",
    "                # layers 2-3\n",
    "    Z = convolve_forward(b, b.images, M.W2, L=2)\n",
    "    Zn = batchnorm_forward(M, b, Z, L=2)\n",
    "    A = ReLU(Zn)  \n",
    "    A = maxpool_forward(b, inpt=A, L=3)\n",
    "                # layers 4-5    \n",
    "    Z = convolve_forward(b, A, M.W4, L=4)\n",
    "    Zn = batchnorm_forward(M, b, Z, L=4)\n",
    "    A = ReLU(Zn)  \n",
    "    A = maxpool_forward(b, inpt=A, L=5)\n",
    "                # layers 6-7\n",
    "    Z = convolve_forward(b, A, M.W6, L=6)\n",
    "    Zn = batchnorm_forward(M, b, Z, L=6)\n",
    "    A = ReLU(Zn)  \n",
    "    A = maxpool_forward(b, inpt=A, L=7)\n",
    "    for i in range(b.size):\n",
    "        b.A7flat[i] = np.ravel( A[i] )\n",
    "                # layer 8        \n",
    "    Z = b.A7flat @ M.W8\n",
    "    Zn = batchnorm_forward(M, b, Z, L=8)\n",
    "    if drp: Zn = dropout_forward(b, Zn, L=8)   \n",
    "    b.A8 = ReLU(Zn)                   \n",
    "                # layer 9    \n",
    "    Z = b.A8 @ M.W9\n",
    "    Zn = batchnorm_forward(M, b, Z, L=9)\n",
    "    if drp: Zn = dropout_forward(b, Zn, L=9) \n",
    "    if f == Softmax: b.yHat = Softmax(Zn)   \n",
    "    if f == SVM:     b.SVMloss = SVM(b, Zn)\n",
    "        \n",
    "def dropout_backward(b, backerr, L):\n",
    "    '''Applies dropout to backerr1 by multiplying it with droput mask, U.'''\n",
    "    if L == 9: return backerr * b.U9\n",
    "    if L == 8: return backerr * b.U8\n",
    "    \n",
    "def backward(M, b, d_function, drp=True):\n",
    "    '''Backward steps as noted in chart. Input: e.g., dSoftmax, or dSVM.'''\n",
    "              # layer 9\n",
    "    backerr = d_function(b)\n",
    "    if drp: backerr = dropout_backward(b, backerr, L=9)  \n",
    "    backerr, dgamma9, dbeta9 = batchnorm_backward(M, b, backerr, L=9)\n",
    "    dW9 = b.A8.T @ backerr\n",
    "    outerr = backerr @ M.W9.T\n",
    "              # layer 8    \n",
    "    backerr = d_ReLU(outerr, b.Zn8)\n",
    "    if drp: backerr = dropout_backward(b, backerr, L=8) \n",
    "    backerr, dgamma8, dbeta8 = batchnorm_backward(M, b, backerr, L=8)\n",
    "    dW8 = b.A7flat.T @ backerr\n",
    "    outerr = (backerr @ M.W8.T).reshape(b.A7shape)\n",
    "              # layers 7-6    \n",
    "    outerr = maxpool_backward(b, outerr, L=7)\n",
    "    backerr = d_ReLU(outerr, b.Zn6)\n",
    "    backerr, dgamma6, dbeta6 = batchnorm_backward(M, b, backerr, L=6)    \n",
    "    dW6, outerr = convolve_backward(M, b, backerr, L=6)  \n",
    "              # layers 5-4   \n",
    "    outerr = maxpool_backward(b, outerr, L=5)\n",
    "    backerr = d_ReLU(outerr, b.Zn4)\n",
    "    backerr, dgamma4, dbeta4 = batchnorm_backward(M, b, backerr, L=4)    \n",
    "    dW4, outerr = convolve_backward(M, b, backerr, L=4)   \n",
    "              # layers 3-2\n",
    "    outerr = maxpool_backward(b, outerr, L=3)\n",
    "    backerr = d_ReLU(outerr, b.Zn2)\n",
    "    backerr, dgamma2, dbeta2 = batchnorm_backward(M, b, backerr, L=2)    \n",
    "    dW2, outerr = convolve_backward(M, b, backerr, L=2)\n",
    "    \n",
    "    gradients = [dW2, dW4, dW6, dW8, dW9, dgamma2, dgamma4, dgamma6, \n",
    "                 dgamma8, dgamma9, dbeta2, dbeta4, dbeta6, dbeta8, dbeta9]\n",
    "      \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient check\n",
    "gradient check model is a lighter version of Model class:<br>\n",
    "&nbsp;&nbsp;&nbsp;convolutional layers: &nbsp;&nbsp;layer 2: two neurons &nbsp;&nbsp;&nbsp;&nbsp;layer 4: three neurons &nbsp;&nbsp;&nbsp;&nbsp;layer 6: four neurons<br>\n",
    "\n",
    "batch class is used - pooling activity shapes with respect to lighter network size are as follows:<br>\n",
    "&nbsp;&nbsp;&nbsp;pooling layers: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; layer 3: &nbsp; '2x16x16'&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; layer 5:&nbsp;&nbsp;  '3x8x8'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; layer 7: &nbsp;&nbsp;'4x4x4'<br><br>\n",
    "note: dropout and L2 regularization are omitted from gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gradient_check_model:\n",
    "    def __init__(self): \n",
    "        np.random.seed(1)\n",
    "        self.W2              = 0.1 * np.random.randn( 2, 3, 5, 5) \n",
    "        self.W4              = 0.1 * np.random.randn( 3, 2, 5, 5) \n",
    "        self.W6              = 0.1 * np.random.randn( 4, 3, 5, 5) \n",
    "        self.W8              = 0.1 * np.random.randn( 64, 20 )  \n",
    "        self.W9              = 0.1 * np.random.randn( 20, 10 )   \n",
    "        self.gam2            = np.ones(  (2,32,32) )             # per spatial location \n",
    "        self.gam4            = np.ones(  (3,16,16) )             # \"    \n",
    "        self.gam6            = np.ones(  (4, 8, 8) )             # \"\n",
    "        self.gam8            = np.ones(  (1, self.W8.shape[1]) ) # per hidden neuron     \n",
    "        self.gam9            = np.ones(  (1, self.W9.shape[1]) ) # per output neuron      \n",
    "        self.beta2           = np.zeros( (2,32,32) )      \n",
    "        self.beta4           = np.zeros( (3,16,16) )     \n",
    "        self.beta6           = np.zeros( (4, 8, 8) )     \n",
    "        self.beta8           = np.zeros( (1, self.W8.shape[1]) )     \n",
    "        self.beta9           = np.zeros( (1, self.W9.shape[1]) )     \n",
    "        self.mu2             = np.zeros( (2,32,32) )             \n",
    "        self.mu4             = np.zeros( (3,16,16) )             \n",
    "        self.mu6             = np.zeros( (4, 8, 8) )            \n",
    "        self.mu8             = np.zeros( (1, self.W8.shape[1]) )            \n",
    "        self.mu9             = np.zeros( (1, self.W9.shape[1]) )             \n",
    "        self.var2            = np.zeros( (2,32,32) )             \n",
    "        self.var4            = np.zeros( (3,16,16) )            \n",
    "        self.var6            = np.zeros( (4, 8, 8) )            \n",
    "        self.var8            = np.zeros( (1, self.W8.shape[1]) )             \n",
    "        self.var9            = np.zeros( (1, self.W9.shape[1]) ) \n",
    "        self.ravelshapeW2    = self.W2.ravel().shape[0]              \n",
    "        self.ravelshapegam2  = self.gam2.ravel().shape[0]\n",
    "        self.ravelshapebeta2 = self.beta2.ravel().shape[0]\n",
    "        self.ravelshapeW4    = self.W4.ravel().shape[0]              \n",
    "        self.ravelshapegam4  = self.gam4.ravel().shape[0]\n",
    "        self.ravelshapebeta4 = self.beta4.ravel().shape[0]\n",
    "        self.ravelshapeW6    = self.W6.ravel().shape[0]              \n",
    "        self.ravelshapegam6  = self.gam6.ravel().shape[0]\n",
    "        self.ravelshapebeta6 = self.beta6.ravel().shape[0]\n",
    "        self.ravelshapeW8    = self.W8.ravel().shape[0]              \n",
    "        self.ravelshapegam8  = self.gam8.ravel().shape[0]\n",
    "        self.ravelshapebeta8 = self.beta8.ravel().shape[0]\n",
    "        self.ravelshapeW9    = self.W9.ravel().shape[0]              \n",
    "        self.ravelshapegam9  = self.gam9.ravel().shape[0]\n",
    "        self.ravelshapebeta9 = self.beta9.ravel().shape[0]\n",
    "        \n",
    "def shape_gradient_batch(b):\n",
    "    b.imgshp   = (b.size,   3, 32, 32)      \n",
    "    b.A3shape  = (b.size,   2, 16, 16)       \n",
    "    b.A5shape  = (b.size,   3,  8,  8)                                  \n",
    "    b.A7shape  = (b.size,   4,  4,  4)       \n",
    "    b.A7flat   = np.zeros( (b.size, 64) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient check (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setWgb(m, Wgb):\n",
    "    W2_end     =         0 + m.ravelshapeW2\n",
    "    gam2_end   =    W2_end + m.ravelshapegam2\n",
    "    beta2_end  =  gam2_end + m.ravelshapebeta2\n",
    "    W4_end     = beta2_end + m.ravelshapeW4\n",
    "    gam4_end   =    W4_end + m.ravelshapegam4\n",
    "    beta4_end  =  gam4_end + m.ravelshapebeta4\n",
    "    W6_end     = beta4_end + m.ravelshapeW6\n",
    "    gam6_end   =    W6_end + m.ravelshapegam6\n",
    "    beta6_end  =  gam6_end + m.ravelshapebeta6\n",
    "    W8_end     = beta6_end + m.ravelshapeW8\n",
    "    gam8_end   =    W8_end + m.ravelshapegam8\n",
    "    beta8_end  =  gam8_end + m.ravelshapebeta8\n",
    "    W9_end     = beta8_end + m.ravelshapeW9\n",
    "    gam9_end   =    W9_end + m.ravelshapegam9\n",
    "    beta9_end  =  gam9_end + m.ravelshapebeta9    \n",
    "    m.W2       = np.reshape( Wgb[        0:W2_end],       m.W2.shape )\n",
    "    m.gam2     = np.reshape( Wgb[   W2_end:gam2_end],   m.gam2.shape )\n",
    "    m.beta2    = np.reshape( Wgb[ gam2_end:beta2_end], m.beta2.shape )\n",
    "    m.W4       = np.reshape( Wgb[beta2_end:W4_end],       m.W4.shape )\n",
    "    m.gam4     = np.reshape( Wgb[   W4_end:gam4_end],   m.gam4.shape )\n",
    "    m.beta4    = np.reshape( Wgb[ gam4_end:beta4_end], m.beta4.shape )\n",
    "    m.W6       = np.reshape( Wgb[beta4_end:W6_end],       m.W6.shape ) \n",
    "    m.gam6     = np.reshape( Wgb[   W6_end:gam6_end],   m.gam6.shape )\n",
    "    m.beta6    = np.reshape( Wgb[ gam6_end:beta6_end], m.beta6.shape )\n",
    "    m.W8       = np.reshape( Wgb[beta6_end:W8_end],       m.W8.shape )  \n",
    "    m.gam8     = np.reshape( Wgb[   W8_end:gam8_end],   m.gam8.shape )\n",
    "    m.beta8    = np.reshape( Wgb[ gam8_end:beta8_end], m.beta8.shape )\n",
    "    m.W9       = np.reshape( Wgb[beta8_end:W9_end],       m.W9.shape )      \n",
    "    m.gam9     = np.reshape( Wgb[   W9_end:gam9_end],   m.gam9.shape )\n",
    "    m.beta9    = np.reshape( Wgb[ gam9_end:beta9_end], m.beta9.shape )\n",
    "    \n",
    "def costFunction(m, b, f):\n",
    "    forward(m, b, f, drp=False)\n",
    "    if f == Softmax:\n",
    "        logloss = -np.log( b.yHat[b.imap, b.Y] ) \n",
    "        return np.sum(logloss)\n",
    "    if f == SVM:\n",
    "        return np.sum(b.SVMloss) \n",
    "    \n",
    "def computeNumericalGradients(m, b, activation, h=1e-5):\n",
    "    '''Computes numerical gradient using Taylor expansion of 'f(x+h) and f(x-h)'.'''\n",
    "    Wgb = np.concatenate( (m.W2.ravel(), m.gam2.ravel(), m.beta2.ravel(),\n",
    "                           m.W4.ravel(), m.gam4.ravel(), m.beta4.ravel(),\n",
    "                           m.W6.ravel(), m.gam6.ravel(), m.beta6.ravel(),\n",
    "                           m.W8.ravel(), m.gam8.ravel(), m.beta8.ravel(),\n",
    "                           m.W9.ravel(), m.gam9.ravel(), m.beta9.ravel()) )\n",
    "    numerical_gradients = np.zeros_like(Wgb)\n",
    "    v = np.zeros_like(Wgb)\n",
    "    endrange = Wgb.shape[0] \n",
    "    for i in range(endrange):\n",
    "        v[i] = h\n",
    "        setWgb(m, Wgb+v)                     \n",
    "        loss2 = costFunction(m, b, activation)\n",
    "        setWgb(m, Wgb-v)\n",
    "        loss1 = costFunction(m, b, activation)\n",
    "        numerical_gradients[i] = (loss2 - loss1) / (2 * h)\n",
    "        v[i] = 0\n",
    "    setWgb(m, Wgb) \n",
    "    return numerical_gradients\n",
    "\n",
    "def computeAnalyticGradients(m, b, f, delta_f):\n",
    "    '''Run forward/backward pass, unpack gradients, & reformat for numerical comparison.'''\n",
    "    forward(m, b, f, drp=False) \n",
    "    g = backward(m, b, delta_f, drp=False)\n",
    "    dW2, dW4, dW6, dW8, dW9 = g[0],   g[1],  g[2],  g[3],  g[4] \n",
    "    dg2, dg4, dg6, dg8, dg9 = g[5],   g[6],  g[7],  g[8],  g[9]\n",
    "    db2, db4, db6, db8, db9 = g[10], g[11], g[12], g[13], g[14]\n",
    "    return np.concatenate( (dW2.ravel(), dg2.ravel(), db2.ravel(), dW4.ravel(), dg4.ravel(), \n",
    "                            db4.ravel(), dW6.ravel(), dg6.ravel(), db6.ravel(), dW8.ravel(), \n",
    "                            dg8.ravel(), db8.ravel(), dW9.ravel(), dg9.ravel(), db9.ravel()) )\n",
    "\n",
    "def get_init_model(m):\n",
    "    return ( [m.W2, m.W4, m.W6, m.W8, m.W9, m.gam2, m.gam4, m.gam6, m.gam8, m.gam9, \n",
    "              m.beta2, m.beta4, m.beta6, m.beta8, m.beta9]  )          \n",
    "\n",
    "def compare_gradients(m, b, activation_function, delta_function):\n",
    "    '''Computes numerical gradients, and analytic gradients, then compares both. An initial \n",
    "       parameter update is applied (since beta parameters are initialized as zeros). '''\n",
    "    shape_gradient_batch(b)\n",
    "    b.images, b.labels = d.XTrn[:b.size], d.yTrn[:b.size]\n",
    "    b.Y = np.argmax(b.labels, axis=1)\n",
    "    f, delta_f = activation_function, delta_function\n",
    "    forward(m, b, f, drp=False)\n",
    "    gradients = backward(m, b, delta_f, drp=False)\n",
    "    mdl = get_init_model(m)  \n",
    "    for param in range(len(mdl)):\n",
    "        mdl[param] -= 0.001 * gradients[param]\n",
    "    numgrads = computeNumericalGradients(m, b, f) \n",
    "    grads = computeAnalyticGradients(m, b, f, delta_f)\n",
    "    print ('result should be on the order of 1e-8 or less:')\n",
    "    print ( np.linalg.norm(grads-numgrads) / np.linalg.norm(grads+numgrads) )\n",
    "    return numgrads, grads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient check (continued) -  Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result should be on the order of 1e-8 or less:\n",
      "2.23935001674e-09\n"
     ]
    }
   ],
   "source": [
    "m, b = gradient_check_model(), Batch(size=8)   \n",
    "numgrads, grads = compare_gradients(m, b, Softmax, d_Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient check (continued) - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result should be on the order of 1e-8 or less:\n",
      "3.8654875219e-10\n"
     ]
    }
   ],
   "source": [
    "m, b = gradient_check_model(), Batch(size=8)   \n",
    "numgrads, grads = compare_gradients(m, b, SVM, d_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions\n",
    "continued from primary forward and backward pass functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_loss(M):\n",
    "    Ws = [M.W2, M.W4, M.W6, M.W8, M.W9]\n",
    "    loss = 0\n",
    "    for W in Ws:\n",
    "        loss += 0.5 * M.lmbda * np.sum(W*W) \n",
    "    return loss\n",
    "\n",
    "def train_step(M, b, f):\n",
    "    '''A single training step: forward, loss, backprop'''\n",
    "    forward(M, b, f)\n",
    "    if f == Softmax:\n",
    "        logloss = -np.log( b.yHat[b.imap, b.Y] ) \n",
    "        loss = np.sum(logloss) / b.size  +  reg_loss(M) / b.size\n",
    "        gradients = backward(M, b, d_Softmax)\n",
    "    if f == SVM:\n",
    "        loss = np.sum(b.SVMloss) / b.size  +  reg_loss(M) / b.size\n",
    "        gradients = backward(M, b, d_SVM)\n",
    "    return gradients, loss\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "def forward_val_tst(M, b):\n",
    "    '''Forward steps streamlined for validation/testing.   Note: Softmax or SVM \n",
    "       activation is not required for accuracy testing.  Returns Z-normalized.'''\n",
    "    Z = convolve_forward(b, b.images, M.W2, L=2, test=True)  \n",
    "    Zn = batchnorm_forward(M, b, Z, L=2, test=True)\n",
    "    A = maxpool_forward(b, ReLU(Zn), L=3, test=True)              \n",
    "    Z = convolve_forward(b, A, M.W4, L=4, test=True)     \n",
    "    Zn = batchnorm_forward(M, b, Z, L=4, test=True)\n",
    "    A = maxpool_forward(b, ReLU(Zn), L=5, test=True)              \n",
    "    Z = convolve_forward(b, A, M.W6, L=6, test=True)     \n",
    "    Zn = batchnorm_forward(M, b, Z, L=6, test=True)\n",
    "    A = maxpool_forward(b, ReLU(Zn), L=7, test=True)          \n",
    "    for i in range(b.size):\n",
    "        b.A7flat[i] = np.ravel( A[i])\n",
    "    Z = b.A7flat @ M.W8                                  \n",
    "    Zn = batchnorm_forward(M, b, Z, L=8, test=True)\n",
    "    Z = ReLU(Zn) @ M.W9                                             \n",
    "    return batchnorm_forward(M, b, Z, L=9, test=True)\n",
    "\n",
    "def display_progress(d, M, b, loss, t):\n",
    "    '''Makes predictions with respect to validation data, and reports accuracy based on \n",
    "       the class with highest score.'''\n",
    "    b = Batch( size=len(d.XVal) ) \n",
    "    b.images, b.labels = d.XVal, d.yVal\n",
    "    scores = forward_val_tst(M, b)\n",
    "    val_acc = accuracy( np.argmax(b.labels, axis=1), np.argmax(scores, axis=1) )   \n",
    "    print('Iter-{} loss: {:.3f} accuracy: {:3f}'.format(t, loss, val_acc))\n",
    "                     \n",
    "def retrieve_next_batch(d, size):\n",
    "    X, y = skshuffle(d.XTrn, d.yTrn)\n",
    "    images, labels = X[:size], y[:size] \n",
    "    return images, labels, np.argmax(labels, axis=1)\n",
    "    \n",
    "def get_init_model(M):\n",
    "    return ( [M.W2, M.W4, M.W6, M.W8, M.W9, M.gam2, M.gam4, M.gam6, M.gam8, M.gam9, \n",
    "              M.beta2, M.beta4, M.beta6, M.beta8, M.beta9]  )   \n",
    "\n",
    "# instantiate original batch class\n",
    "b = Batch(size=128)\n",
    "b.imgshp   = (b.size,   3, 32, 32)                  \n",
    "b.A3shape  = (b.size,  32, 16, 16)            \n",
    "b.A5shape  = (b.size,  64,  8,  8)                                \n",
    "b.A7shape  = (b.size, 128,  4,  4)             \n",
    "b.A7flat   = np.zeros((b.size, 2048))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgd\n",
    "𝜂 &nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;learnrate<br>\n",
    "𝑡 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;iteration<br>\n",
    "∂𝑤 &nbsp;:&nbsp;gradient<br><br>\n",
    "𝑤<sub>𝑡+1</sub> &nbsp; ← &nbsp; 𝑤<sub>𝑡</sub> &nbsp; - &nbsp; 𝜂 &nbsp;⋅ &nbsp;∂𝑤"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for param in range(len(model)):\n",
    "    model[param] += - lrate * gradients[param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sgd with momentum\n",
    "𝜌 &nbsp;: friction constant <br>\n",
    "𝑣 &nbsp;: velocity, a moving average of gradients with greater emphasis on recent gradient<br>\n",
    "\n",
    "𝑣<sub>𝑡+1</sub> &nbsp; ← &nbsp; 𝜌 &nbsp; ⋅ &nbsp;𝑣<sub>𝑡</sub>&nbsp; + &nbsp;∂𝑤<br>\n",
    "𝑤<sub>𝑡+1</sub> &nbsp; ← &nbsp;𝑤<sub>𝑡</sub> &nbsp;- &nbsp;𝜂&nbsp; ⋅ &nbsp;𝑣"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "velocity = np.zeros_like(model)\n",
    "for param in range(len(gradients)):\n",
    "    velocity[param] = const * velocity[param] + gradients[param]\n",
    "    model[param] += - lrate * v[param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rms prop\n",
    "weights that receive high gradients will have their effective learning rate reduced, while weights that receive<br> small or infrequent updates will have their effective learning rate increased<br><br>\n",
    "*𝛾* : decay rate constant<br>\n",
    "𝑣 : cached variable that keeps track of per-parameter sum of squared gradients<br>\n",
    "𝜀 : avoids division by zero<br>\n",
    "\n",
    "𝑣<sub>𝑡+1</sub> &nbsp; ← &nbsp; *𝛾* &nbsp; ⋅ &nbsp;𝑣<sub>𝑡</sub> &nbsp;+&nbsp; (𝟙 - *𝛾*) &nbsp;⋅ &nbsp;∂𝑤<sup>2</sup><br>\n",
    "𝑤<sub>𝑡+1</sub> &nbsp; ← &nbsp;𝑤<sub>𝑡</sub> &nbsp;- &nbsp;𝜂 &nbsp;/ &nbsp;( 𝑠𝑞𝑟𝑡(𝑣) &nbsp;+&nbsp; 𝜀 )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "v = np.zeros_like(model)\n",
    "for param in range(len(gradients)):\n",
    "    v[param] += const * v[param] + (1. - const) * gradients[param]**2\n",
    "    model[param] += - lrate / ( np.sqrt(v[param]) + eps )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adam optimizer\n",
    "𝛽<sub>1</sub>, 𝛽<sub>2</sub>&nbsp;&nbsp;:&nbsp;decay constants<br>\n",
    "𝑚 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: cached variable that keeps track of per-parameter sum of gradients&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;𝑚<sub>𝑡+1</sub> &nbsp; ← &nbsp; 𝛽<sub>1</sub> &nbsp;  ⋅ &nbsp;𝑚<sub>𝑡</sub> &nbsp;+&nbsp; ( 𝟙 - 𝛽<sub>1</sub> ) &nbsp;⋅ &nbsp;∂𝑤<br>\n",
    "𝑣 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: cached variable that keeps track of per-parameter sum of squared gradients&nbsp;&nbsp;&nbsp;𝑣<sub>𝑡+1</sub> &nbsp; ← &nbsp; 𝛽<sub>2</sub> &nbsp;&nbsp;⋅ &nbsp;𝑣<sub>𝑡</sub> &nbsp; +&nbsp; ( 𝟙 - 𝛽<sub>2</sub> ) &nbsp;⋅ &nbsp;∂𝑤<sup>2</sup><br>\n",
    "𝑚ℎ𝑎𝑡 &nbsp; :&nbsp;bias correction<sub>1</sub>&nbsp;&nbsp;&nbsp;&nbsp;𝑚ℎ𝑎𝑡<sub>𝑤</sub> &nbsp;←&nbsp; 𝑚<sub>𝑤</sub><sup>(𝑡+1)</sup> / ( 𝟙 - 𝛽<sub>1</sub><sup>𝑡</sup> )<br>\n",
    "𝑣ℎ𝑎𝑡 &nbsp; : &nbsp;bias correction<sub>2</sub>&nbsp;&nbsp;&nbsp;&nbsp;𝑣ℎ𝑎𝑡<sub>𝑤</sub> &nbsp;←&nbsp; 𝑣<sub>𝑤</sub><sup>(𝑡+1)</sup>&nbsp; / ( 𝟙 - 𝛽<sub>2</sub><sup>𝑡</sup> )<br><br>\n",
    "𝑤<sub>𝑡+1</sub> &nbsp; ← &nbsp; 𝑤<sub>𝑡</sub> &nbsp; - &nbsp; 𝜂 &nbsp; ⋅ &nbsp;𝑚ℎ𝑎𝑡 &nbsp; / &nbsp; ( 𝑠𝑞𝑟𝑡(𝑣ℎ𝑎𝑡) &nbsp;+&nbsp; 𝜀 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 loss: 2.059 accuracy: 0.462891\n",
      "Iter-200 loss: 1.957 accuracy: 0.500977\n",
      "Iter-300 loss: 1.986 accuracy: 0.555664\n",
      "Iter-400 loss: 1.723 accuracy: 0.540039\n",
      "Iter-500 loss: 1.680 accuracy: 0.567383\n",
      "Iter-600 loss: 1.655 accuracy: 0.594727\n",
      "Iter-700 loss: 1.660 accuracy: 0.597656\n",
      "Iter-800 loss: 1.590 accuracy: 0.573242\n",
      "Iter-900 loss: 1.562 accuracy: 0.543945\n",
      "Iter-1000 loss: 1.649 accuracy: 0.550781\n",
      "Iter-1100 loss: 1.709 accuracy: 0.488281\n",
      "Iter-1200 loss: 1.596 accuracy: 0.652344\n",
      "Iter-1300 loss: 1.456 accuracy: 0.641602\n",
      "Iter-1400 loss: 1.613 accuracy: 0.631836\n",
      "Iter-1500 loss: 1.622 accuracy: 0.616211\n",
      "Iter-1600 loss: 1.520 accuracy: 0.499023\n",
      "Iter-1700 loss: 1.587 accuracy: 0.603516\n",
      "Iter-1800 loss: 1.487 accuracy: 0.677734\n",
      "Iter-1900 loss: 1.312 accuracy: 0.648438\n",
      "Iter-2000 loss: 1.416 accuracy: 0.599609\n"
     ]
    }
   ],
   "source": [
    "def adam(d, M, b, f, lrate=1e-3, n_iter=2000, eps=1e-8, print_after=100):\n",
    "    const1, const2 =.9, .999\n",
    "    mdl = get_init_model(M) \n",
    "    m, v = np.zeros_like(mdl), np.zeros_like(mdl)\n",
    "    for t in range(1, n_iter+1):\n",
    "        b = Batch()\n",
    "        b.images, b.labels, b.Y = retrieve_next_batch(d, b.size)\n",
    "        gradients, loss = train_step(M, b, f)\n",
    "        if t % print_after == 0: \n",
    "            display_progress(d, M, b, loss, t)\n",
    "        for w in range(M.numW):\n",
    "            gradients[w] += M.lmbda * mdl[w]\n",
    "        for param in range(len(gradients)):\n",
    "            m[param] = const1 * m[param] + (1. - const1) * gradients[param]\n",
    "            v[param] = const2 * v[param] + (1. - const2) * gradients[param]**2\n",
    "            mhat = m[param] / ( 1 - const1**(t) )    \n",
    "            vhat = v[param] / ( 1 - const2**(t) )\n",
    "            mdl[param] += - lrate * mhat / ( np.sqrt(vhat) + eps )\n",
    "    return mdl\n",
    "\n",
    "M = Model()\n",
    "adm = adam(d, M, b, Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chart of forward/backward steps (through layer 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FORWARD PROP BEGINS HERE \t\t   LAYER 2               BACKWARD PROP ENDS HERE\n",
    "            ⇓                 ----------------                     ↑       \n",
    "    Z{L} = A{L-1} * W         |  CONVOLUTION  |        outerr{L-1} = δ2{L} * W.T{L}\t       \n",
    "                              |      gate     |          ∂W{L} = A.T{L-1} * δ2{L}\t\t\n",
    "    \t    ⇓                 ----------------                     ↑\n",
    "   Znorm = bn_forward(Z)\t  | BATCHNORM gate|      δ2, ∂gamma, ∂beta = bn_backwd(δ1)\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "     A = ReLU(Znorm)\t      |\t  ReLU gate\t  |       δ1 = outerr * ReLUprime(Znorm)\t       \n",
    "            ⇓                 ----------------                     ↑  \n",
    "            \t\t\t    \t   LAYER 3                         \n",
    "            ⇓                 ----------------                     ↑\n",
    "  in: A{L-1} → out: A{L}      | MAX POOL gate |      in: outerr{L} → out: outerr{L-1} \n",
    "            ⇓                 ----------------                     ↑\n",
    "                                   LAYER 4\n",
    "            ⇓                 ----------------                     ↑         \n",
    "     Z{L} = A{L-1} * W        |  CONVOLUTION  |        outerr{L-1} = δ2{L} * W.T{L}\t       \n",
    "                              |      gate     |          ∂W{L} = A.T{L-1} * δ2{L}\t\t\n",
    "    \t    ⇓                 ----------------                     ↑\n",
    "   Znorm = bn_forward(Z)\t  | BATCHNORM gate|      δ2, ∂gamma, ∂beta = bn_backwd(δ1)\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "     A = ReLU(Znorm)\t      |\t  ReLU gate\t  |       δ1 = outerr * ReLUprime(Znorm)\t       \n",
    "            ⇓                 ----------------                     ↑\n",
    "                                                         BACKWARD PROP (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chart of forward/backward steps (layers 5-7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FORWARD PROP(continued)            LAYER 5\n",
    "            ⇓                 ----------------                     ↑\n",
    "  in: A{L-1} → out: A{L}      | MAX POOL gate |      in: outerr{L} → out: outerr{L-1} \n",
    "            ⇓                 ----------------                     ↑\n",
    "                                   LAYER 6\n",
    "            ⇓                 ----------------                              \n",
    "    Z{L} = A{L-1} * W         |  CONVOLUTION  |        outerr{L-1} = δ2{L} * W.T{L}\t       \n",
    "                              |      gate     |          ∂W{L} = A.T{L-1} * δ2{L}\t\t\n",
    "    \t    ⇓                 ----------------                     ↑\n",
    "   Znorm = bn_forward(Z)\t  | BATCHNORM gate|      δ2, ∂gamma, ∂beta = bn_backwd(δ1)\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "     A = ReLU(Znorm)\t      |\t  ReLU gate\t  |       δ1 = outerr * ReLUprime(Znorm)\n",
    "            ⇓                 ----------------                     ↑\n",
    "            \t\t\t    \t   LAYER 7\n",
    "            ⇓                 ----------------                     ↑\n",
    "  in: A{L-1} → out: A{L}      | MAX POOL gate |      in: outerr{L} → out: outerr{L-1} \n",
    "            ⇓                 ----------------                     ↑\n",
    "        flatten A             |flatten/reshape|              reshape outerr \n",
    "            ⇓                 ----------------                     ↑\n",
    "                                                        BACKWARD PROP (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chart of forward/backward steps (layers 8-9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FORWARD PROP(continued)            LAYER 8\n",
    "            ⇓                 ----------------                     ↑       \n",
    "    Z{L} = A{L-1} * W         |FULLY-CONNECTED|        outerr{L-1} = δ2{L} * W.T{L}\t       \n",
    "                              |     gate      |          ∂W{L} = A.T{L-1} * δ2{L}\t\t\n",
    "    \t    ⇓                 ----------------                     ↑\n",
    "  Znorm  = bn_forward(Z)\t  | BATCHNORM gate|      δ2, ∂gamma, ∂beta = bn_backwd(δ1)\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "    Znorm = Znorm * U\t      |  DROPOUT gate |               δ1 = δ1 * U\t\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "     A = ReLU(Znormd)\t      |\t  ReLU gate\t  |       δ1 = outerr * ReLUprime(Znorm)\t       \n",
    "            ⇓                 ----------------                     ↑\n",
    "                                   LAYER 9\n",
    "            ⇓                 ----------------                     ↑       \n",
    "    Z{L} = A{L-1} * W         |FULLY-CONNECTED|        outerr{L-1} = δ2{L} * W.T{L}\t       \n",
    "                              |     gate      |          ∂W{L} = A.T{L-1} * δ2{L}\t\t\n",
    "    \t    ⇓                 ----------------                     ↑\n",
    "  Znorm = bn_forward(Z)\t      | BATCHNORM gate|      δ2, ∂gamma, ∂beta = bn_backwd(δ1)\n",
    "            ⇓                 ----------------                     ↑                  \n",
    "     Znorm = Znorm * U\t      |  DROPOUT gate |                δ1 = δ1 * U\t\n",
    "            ⇓                 ----------------                     ↑ \n",
    "  yhat = Softmax(Znorm)       |  SOFTMAX gate |     δ1 = Softmax delta function(y, yhat)      \n",
    "            ⇓                 ----------------                     ↑\n",
    "  FORWARD PROP ENDS HERE      | COMPUTATION OF|          May be skipped as Softmax             \n",
    "                              |  OUTPUT ERROR |        delta function will compute δ  \n",
    "            ⇓                 ----------------                     ↑\n",
    "             ⥤                                          BACKWARD PROP BEGINS HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivation of backpropogation formulas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; J &nbsp; = &nbsp; <sup>𝟷</sup>∕<sub>𝟸</sub> ⋅ ( *𝑦 - 𝑦ℎ𝑎𝑡* )<sup>𝟸</sup>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;∂J/∂𝑤 &nbsp; = &nbsp;&nbsp; ( *𝑦 - 𝑦ℎ𝑎𝑡* ) &nbsp;&nbsp; ⋅ &nbsp;&nbsp;   ( ∂*𝑦ℎ𝑎𝑡* / ∂𝑤 )<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; = &nbsp;&nbsp; ( *𝑦 - 𝑦ℎ𝑎𝑡* ) &nbsp;&nbsp; ⋅ &nbsp;&nbsp;   ( ∂*𝑦ℎ𝑎𝑡* / ∂𝑧 ) &nbsp;&nbsp; ⋅ &nbsp;&nbsp;   ( ∂𝑧 / ∂𝑤 )<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; = &nbsp;&nbsp;&nbsp;&nbsp;  𝑜𝑢𝑡𝑒𝑟𝑟<sub>𝐿</sub>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⋅ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    σ’(𝑧<sub> 𝐿</sub> )  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⋅ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a<sub> 𝐿 - 𝟙</sub><br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; δ &nbsp; = &nbsp;&nbsp;&nbsp;&nbsp; 𝑜𝑢𝑡𝑒𝑟𝑟  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⋅ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    σ’(𝑧)<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ∂J/∂𝑤<sub>*L* </sub> &nbsp;= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; δ<sub> *L*</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⋅ &nbsp;&nbsp;&nbsp;&nbsp;   a<sup>T</sup><sub>*L - 𝟙*</sub><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;𝑜𝑢𝑡𝑒𝑟𝑟<sub>*L-1*</sub> &nbsp;&nbsp; = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; δ<sub> *L*</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⋅ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   𝑤<sup>T</sup><sub>𝐿</sub><br><br>\n",
    "σ is activation function<br>\n",
    "σ’ is prime activation function<br>\n",
    "𝑤,&nbsp; &nbsp;∂J/∂𝑤&nbsp; &&nbsp; δ&nbsp; &nbsp;do not apply to pooling layers<br>\n",
    "δ is backpropgating error<br>\n",
    "∂W is used for ∂J/∂𝑤 in above chart, and is referred to in code as dW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes pertaining to use of 'batchnorm' vs 'not using batchnorm'\n",
    "When batch normalization is applied, less focus is needed with respect to the initialization of weights. If  batchnorm <br> was not to be applied, a Xavier initialization of weights should be considered as a way of calibrating variances <br>between network layers. \n",
    "\n",
    "For example:<br>\n",
    "    W2  = np.random.randn(32,  3,  5, 5) \\* np.sqrt(2.0 / (3\\*5\\*5))<br>\n",
    "    W4  = np.random.randn(64,  32, 5, 5) \\* np.sqrt(2.0 / (32\\*5\\*5))<br>\n",
    "    W6  = np.random.randn(128, 64, 5, 5) \\* np.sqrt(2.0 / (64\\*5\\*5))<br>\n",
    "    W8  = np.random.randn(2048, 256) \\* np.sqrt(2.0 / 2048)<br>\n",
    "    W9  = np.random.randn(256,  10)  \\* np.sqrt(2.0 / 256)\n",
    "    \n",
    "ReLU is a common activation function applied in convolutional and fully-connected layers (except for output layer) <br> however, if batch normalization is not applied, 'Leaky ReLU' should be considered if a solution for dying ReLU is <br>needed.   \n",
    "    \n",
    "Biases are used in neural networks as shift parameters, however, if batch normalization is applied, biases may be<br> replaced by gamma and beta parameters.  Bias parameters would be coded, when applicable, as follows:<br>\n",
    "* initialized for conv layer as np.zeros( (self.W{L}.shape[0], 1) ), or fc layer as np.zeros( (1, self.W{L}.shape[1]) ) \n",
    "* included in activity Z computation, e.g., Z{L} = W{L} * A{L-1} + bias{L}\n",
    "* bias gradient is computed in convolution backward pass as 'db = np.sum(backerr, axis=(0, 2, 3))'\n",
    "* ...and 'db.reshape(Wshape0, -1)'\n",
    "* in primary backward pass function as:  'db = np.sum(backerr, axis=0, keepdims=True)'\n",
    "\n",
    "Finally, if batch normalization is not applied, references to batchnorm forward, and batchnorm backward, as well<br> as 'Z-n' should be removed from primary forward and backward pass functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
